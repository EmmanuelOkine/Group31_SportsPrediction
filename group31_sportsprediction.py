# -*- coding: utf-8 -*-
"""Group31_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OJ86HxDikVirCU5ke11ROxno2EmEsmsd
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor
import pickle
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

players21=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/players_21.csv')
players22=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/players_22.csv')

"""### data preparation & feature extraction process"""

players21.head()

players21.shape

players21.info()

players21.describe()

"""DROPPING NULL VALUES FOR 30 PERCENT"""

threshold = len(players21) * 0.3
players21_clean = players21.dropna(thresh=threshold, axis=1)
print("Shape of cleaned dataset:", players21_clean.shape)

players21.fillna(players21.mean(),inplace=True)

correlation = players21.corr()
correlation

independent_features= players21[['overall']]

categorical_columns = players21.select_dtypes(exclude=['int', 'float']).columns.tolist()

encoder = OneHotEncoder(sparse=False, drop='first')
encoder.fit(players21[categorical_columns])
encoded_data = encoder.transform(players21[categorical_columns])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))

correlation = players21.corr()
correlation_target = abs(correlation["overall"])
relevant_features = correlation_target[(correlation_target >= 0.5) & (correlation_target<1)]
relevant_features

# Relevant features based on correlation analysis
relevant_features = [
    'potential', 'value_eur', 'wage_eur', 'release_clause_eur',
    'passing', 'dribbling', 'attacking_short_passing',
    'movement_reactions', 'power_shot_power', 'mentality_vision',
    'mentality_composure'
]

Select_relevant_features= players21[relevant_features]
Select_relevant_features

scaler_standard = StandardScaler()
scaler_standard.fit(players21[relevant_features])
players21_features = scaler_standard.transform(players21[relevant_features])

with open('scaler.pkl', 'wb') as model_file:
    pickle.dump(scaler_standard, model_file)

X = players21[relevant_features]
y = players21['overall']

print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""MODEL TESTING"""

# Train Individual Models
rf = RandomForestRegressor(n_estimators=100, random_state=42)
gb = GradientBoostingRegressor(n_estimators=100)
lr = LinearRegression()

rf.fit(X_train, y_train)
gb.fit(X_train, y_train)
lr.fit(X_train, y_train)

# Soft Voting Ensemble
ensemble = VotingRegressor(estimators=[
    ('rf', rf),
    ('gb', gb),
    ('lr', lr)
])
ensemble.fit(X_train, y_train)

# Predict and Evaluate
y_ensemble_pred = ensemble.predict(X_test)
mae = mean_absolute_error(y_test, y_ensemble_pred)
print(f"Soft Voting Ensemble MAE: {mae}")

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_predictions = rf.predict(X_test)
rf_mae = mean_absolute_error(rf_predictions, y_test)
print("RandomForestRegressor MAE:", rf_mae)
print("RandomForestRegressor RMSE:", np.sqrt(mean_squared_error(rf_predictions, y_test)))

gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
gb_predictions = gb.predict(X_test)
gb_mae = mean_absolute_error(gb_predictions, y_test)

print("GradientBoostingRegressor MAE:", gb_mae)
print("GradientBoostingRegressor RMSE:", np.sqrt(mean_squared_error(gb_predictions, y_test)))

xgb = XGBRegressor(n_estimators=100, random_state=42)
xgb.fit(X_train, y_train)

# Evaluate the model
y_pred = xgb.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("XGBoost MAE:", mae)
print("XGBoost RMSE:", rmse)

# Save the XGBoost model
with open('xgboost_model.pkl', 'wb') as model_file:
    pickle.dump(xgb, model_file)

decision_tree = DecisionTreeRegressor(random_state=42)
decision_tree.fit(X_train, y_train)

# Evaluate the model
y_pred = decision_tree.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("Decision Tree MAE:", mae)
print("Decision Tree RMSE:", rmse)

cv=KFold(n_splits=3)

"""FINE TUNING THE BEST MODEL(RANDOM FOREST REGRESSOR)"""

# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)


grid_search.fit(X_train, y_train)


best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_


best_estimator.fit(X_train, y_train)


y_pred_tuned = best_estimator.predict(X_test)
mae_tuned = mean_absolute_error(y_test, y_pred_tuned)
rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))

print("Tuned RandomForestRegressor MAE:", mae_tuned)
print("Tuned RandomForestRegressor RMSE:", rmse_tuned)
print("Best Parameters:", best_params)

"""DATA CLEANING OF 22 PLAYERS"""

# Define the threshold for dropping columns (30% missing values)
threshold = len(players22) * 0.3

# Drop columns with more than 30% missing values
players22_clean = players22.dropna(thresh=threshold, axis=1)

# Verify the shape of the cleaned dataset
print("Shape of cleaned dataset:", players22_clean.shape)

players22.fillna(players22.mean(), inplace=True)

interest_features = [
    'potential', 'value_eur', 'wage_eur', 'release_clause_eur',
    'passing', 'dribbling', 'attacking_short_passing',
    'movement_reactions', 'power_shot_power', 'mentality_vision',
    'mentality_composure'
]
Select_interest_features = players22[interest_features]
# Standardize the features
scaler_standard = StandardScaler()
players22_features = scaler_standard.fit_transform(players22[interest_features])
players22_features

# Categorical columns (if any)
categorical_columns2 = players22.select_dtypes(exclude=['int', 'float']).columns.tolist()

# Use OneHotEncoder to encode categorical variables
encoder = OneHotEncoder(sparse=False,drop='first')
encoder.fit(players22[categorical_columns2])
encoded_data = encoder.transform(players22[categorical_columns2])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns2))

# Correlation analysis
correlation = players22.corr()
correlation_target = abs(correlation["overall"])
interest_features = correlation_target[(correlation_target >= 0.5) & (correlation_target < 1)]
interest_features

players22.head()

"""TEST ON 22 PLAYERS"""

#Use the trained model to make predictions on players22 data
players22_predictions = grid_search.best_estimator_.predict(X_test)

# Calculate MAE and RMSE for the predictions
mae_players22 = mean_absolute_error(players22_predictions, y_test)
rmse_players22 = np.sqrt(mean_squared_error(players22_predictions, y_test))

print("RandomForestRegressor MAE on players22 data:", mae_players22)
print("RandomForestRegressor RMSE on players22 data:", rmse_players22)

with open('Fifa_prediction.pkl', 'wb') as model_file:
    pickle.dump(best_estimator, model_file)

best_estimator.predict(pd.DataFrame(X_train.iloc[8]).transpose())

